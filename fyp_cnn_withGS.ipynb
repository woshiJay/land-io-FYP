{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10500 images belonging to 21 classes.\n",
      "Found 1050 images belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_image(image_path, size=(224, 224)):\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Quality enhancement\n",
    "    enhancer = ImageEnhance.Sharpness(img)\n",
    "    img = enhancer.enhance(2.0)  # Increase sharpness\n",
    "    \n",
    "    # Noise reduction\n",
    "    image = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Consistency\n",
    "    img = img.resize(size, Image.LANCZOS)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Apply preprocessing and save to new directory\n",
    "input_dir = 'images'\n",
    "output_dir = 'processed_images'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for category in os.listdir(input_dir):\n",
    "    category_path = os.path.join(input_dir, category)\n",
    "    if os.path.isdir(category_path):\n",
    "        output_category_path = os.path.join(output_dir, category)\n",
    "        if not os.path.exists(output_category_path):\n",
    "            os.makedirs(output_category_path)\n",
    "        for image_name in os.listdir(category_path):\n",
    "            image_path = os.path.join(category_path, image_name)\n",
    "            preprocessed_img = preprocess_image(image_path)\n",
    "            preprocessed_img.save(os.path.join(output_category_path, image_name))\n",
    "\n",
    "# Load preprocessed images\n",
    "train_dir = 'processed_images'\n",
    "test_dir = 'images_train_test_val/test'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', init_mode='uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(224, 224, 3)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer=init_mode))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer=init_mode))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=init_mode))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(21, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 09:23:12.552985: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-07-11 09:23:12.553177: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-07-11 09:23:12.553204: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-07-11 09:23:12.553681: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-11 09:23:12.554337: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-07-11 09:23:13.649656: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x38452f1c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3a1d7fc70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "class KerasClassifierGen(KerasClassifier):\n",
    "    def fit(self, x, y, **kwargs):\n",
    "        return self.model.fit(x, y, **kwargs)\n",
    "    \n",
    "    def fit_generator(self, generator, steps_per_epoch, **kwargs):\n",
    "        return self.model.fit(generator, steps_per_epoch=steps_per_epoch, **kwargs)\n",
    "    \n",
    "    def score(self, x, y, **kwargs):\n",
    "        return self.model.evaluate(x, y, **kwargs)[1]\n",
    "\n",
    "#create model\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# parameters\n",
    "param_grid = {\n",
    "    'optimizer': ['SGD', 'Adam'],\n",
    "    'model__init_mode': ['uniform', 'he_normal'],\n",
    "    'epochs': [10, 20],\n",
    "    'batch_size': [10, 20]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
    "\n",
    "# Get a subset of the data\n",
    "X_train, y_train = next(train_generator)\n",
    "X_val, y_val = next(test_generator)\n",
    "\n",
    "\n",
    "# Fit the grid search\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jay/anaconda3/envs/tf_m1_env/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 815ms/step - accuracy: 0.0526 - loss: 7.0410 - val_accuracy: 0.0714 - val_loss: 3.0168\n",
      "Epoch 2/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 626ms/step - accuracy: 0.0527 - loss: 3.0323 - val_accuracy: 0.0495 - val_loss: 3.0430\n",
      "Epoch 3/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 600ms/step - accuracy: 0.0564 - loss: 3.0302 - val_accuracy: 0.0743 - val_loss: 2.8872\n",
      "Epoch 4/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 652ms/step - accuracy: 0.0688 - loss: 2.9661 - val_accuracy: 0.0924 - val_loss: 2.8282\n",
      "Epoch 5/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 653ms/step - accuracy: 0.0737 - loss: 2.9356 - val_accuracy: 0.0981 - val_loss: 2.8229\n",
      "Epoch 6/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 883ms/step - accuracy: 0.0887 - loss: 2.9135 - val_accuracy: 0.1648 - val_loss: 2.7377\n",
      "Epoch 7/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 1s/step - accuracy: 0.1084 - loss: 2.8739 - val_accuracy: 0.1638 - val_loss: 2.7194\n",
      "Epoch 8/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 911ms/step - accuracy: 0.1353 - loss: 2.7642 - val_accuracy: 0.1971 - val_loss: 2.5843\n",
      "Epoch 9/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 1s/step - accuracy: 0.1567 - loss: 2.6561 - val_accuracy: 0.2295 - val_loss: 2.5081\n",
      "Epoch 10/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 1s/step - accuracy: 0.1938 - loss: 2.5531 - val_accuracy: 0.2467 - val_loss: 2.3952\n",
      "Epoch 11/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m847s\u001b[0m 3s/step - accuracy: 0.2191 - loss: 2.4527 - val_accuracy: 0.3114 - val_loss: 2.1542\n",
      "Epoch 12/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 1s/step - accuracy: 0.2710 - loss: 2.2951 - val_accuracy: 0.3390 - val_loss: 2.1355\n",
      "Epoch 13/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 912ms/step - accuracy: 0.2937 - loss: 2.2141 - val_accuracy: 0.3571 - val_loss: 1.9789\n",
      "Epoch 14/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 977ms/step - accuracy: 0.3107 - loss: 2.1527 - val_accuracy: 0.3743 - val_loss: 1.9356\n",
      "Epoch 15/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 1s/step - accuracy: 0.3467 - loss: 2.0508 - val_accuracy: 0.4171 - val_loss: 1.8388\n",
      "Epoch 16/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 1s/step - accuracy: 0.3398 - loss: 1.9950 - val_accuracy: 0.4286 - val_loss: 1.7734\n",
      "Epoch 17/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 1s/step - accuracy: 0.3802 - loss: 1.9390 - val_accuracy: 0.4029 - val_loss: 1.8491\n",
      "Epoch 18/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1672s\u001b[0m 5s/step - accuracy: 0.3983 - loss: 1.8697 - val_accuracy: 0.4495 - val_loss: 1.7101\n",
      "Epoch 19/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5043s\u001b[0m 15s/step - accuracy: 0.4085 - loss: 1.8176 - val_accuracy: 0.4905 - val_loss: 1.6472\n",
      "Epoch 20/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 1s/step - accuracy: 0.4303 - loss: 1.7798 - val_accuracy: 0.4610 - val_loss: 1.6672\n"
     ]
    }
   ],
   "source": [
    "# Retrieve best Keras Classfiier\n",
    "best_params = grid_result.best_params_\n",
    "best_model = create_model(optimizer=best_params['optimizer'], init_mode=best_params['model__init_mode'])\n",
    "\n",
    "history = best_model.fit(\n",
    "    train_generator,\n",
    "    epochs=grid_result.best_params_['epochs'],\n",
    "    validation_data=test_generator,\n",
    "    batch_size=grid_result.best_params_['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'batch_size': 20, 'epochs': 20, 'model__init_mode': 'he_normal', 'optimizer': 'Adam'}\n",
      "Best score: 0.06363636363636364\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 316ms/step - accuracy: 0.0938 - loss: 6.7241\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 319ms/step - accuracy: 0.0969 - loss: 7.6829\n",
      "Train accuracy: 0.0894, Test accuracy: 0.0829\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and results\n",
    "print(f\"Best params: {grid_result.best_params_}\")\n",
    "print(f\"Best score: {grid_result.best_score_}\")\n",
    "\n",
    "# Access the best model\n",
    "best_model = grid_result.best_estimator_.model_\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_acc = best_model.evaluate(train_generator)\n",
    "val_loss, val_acc = best_model.evaluate(test_generator)\n",
    "\n",
    "print(f'Train accuracy: {train_acc:.4f}, Test accuracy: {val_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot training & validation accuracy values\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.06363636363636364 using {'batch_size': 20, 'epochs': 20, 'model__init_mode': 'he_normal', 'optimizer': 'Adam'}\n",
      "0.0 (0.0) with: {'batch_size': 10, 'epochs': 10, 'model__init_mode': 'uniform', 'optimizer': 'SGD'}\n",
      "0.0 (0.0) with: {'batch_size': 10, 'epochs': 10, 'model__init_mode': 'uniform', 'optimizer': 'Adam'}\n",
      "0.03333333333333333 (0.047140452079103175) with: {'batch_size': 10, 'epochs': 10, 'model__init_mode': 'he_normal', 'optimizer': 'SGD'}\n",
      "0.0 (0.0) with: {'batch_size': 10, 'epochs': 10, 'model__init_mode': 'he_normal', 'optimizer': 'Adam'}\n",
      "0.0 (0.0) with: {'batch_size': 10, 'epochs': 20, 'model__init_mode': 'uniform', 'optimizer': 'SGD'}\n",
      "0.030303030303030304 (0.04285495643554834) with: {'batch_size': 10, 'epochs': 20, 'model__init_mode': 'uniform', 'optimizer': 'Adam'}\n",
      "0.030303030303030304 (0.04285495643554834) with: {'batch_size': 10, 'epochs': 20, 'model__init_mode': 'he_normal', 'optimizer': 'SGD'}\n",
      "0.06060606060606061 (0.04285495643554833) with: {'batch_size': 10, 'epochs': 20, 'model__init_mode': 'he_normal', 'optimizer': 'Adam'}\n",
      "0.03333333333333333 (0.047140452079103175) with: {'batch_size': 20, 'epochs': 10, 'model__init_mode': 'uniform', 'optimizer': 'SGD'}\n",
      "0.0 (0.0) with: {'batch_size': 20, 'epochs': 10, 'model__init_mode': 'uniform', 'optimizer': 'Adam'}\n",
      "0.0 (0.0) with: {'batch_size': 20, 'epochs': 10, 'model__init_mode': 'he_normal', 'optimizer': 'SGD'}\n",
      "0.06060606060606061 (0.08570991287109668) with: {'batch_size': 20, 'epochs': 10, 'model__init_mode': 'he_normal', 'optimizer': 'Adam'}\n",
      "0.0 (0.0) with: {'batch_size': 20, 'epochs': 20, 'model__init_mode': 'uniform', 'optimizer': 'SGD'}\n",
      "0.0 (0.0) with: {'batch_size': 20, 'epochs': 20, 'model__init_mode': 'uniform', 'optimizer': 'Adam'}\n",
      "0.03333333333333333 (0.047140452079103175) with: {'batch_size': 20, 'epochs': 20, 'model__init_mode': 'he_normal', 'optimizer': 'SGD'}\n",
      "0.06363636363636364 (0.045150498259852546) with: {'batch_size': 20, 'epochs': 20, 'model__init_mode': 'he_normal', 'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"{mean} ({stdev}) with: {param}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
