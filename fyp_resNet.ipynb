{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow\n",
    "# pip install keras\n",
    "# pip install numpy\n",
    "# pip install matplotlib\n",
    "# pip install pandas\n",
    "# pip install scikit-learn\n",
    "# pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Image Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total corrupted images: 0\n"
     ]
    }
   ],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "def check_images(directory):\n",
    "    corrupted_images = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img.verify()  # This will raise an exception if the image is corrupted\n",
    "            except (UnidentifiedImageError, IOError) as e:\n",
    "                corrupted_images.append(file_path)\n",
    "                print(f\"Corrupted image: {file_path} - {e}\")\n",
    "    return corrupted_images\n",
    "\n",
    "corrupted_images = check_images('images')\n",
    "print(f\"Total corrupted images: {len(corrupted_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, size=(224, 224)):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Quality enhancement\n",
    "        enhancer = ImageEnhance.Sharpness(img)\n",
    "        img = enhancer.enhance(2.0)  # Increase sharpness\n",
    "        \n",
    "        # Noise reduction\n",
    "        image = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "        img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Consistency\n",
    "        img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "        # Normalization\n",
    "        img_array = np.array(img) / 255.0\n",
    "    \n",
    "        # Convert back to image\n",
    "        img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "        \n",
    "        return img\n",
    "    except (UnidentifiedImageError, IOError) as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "input_dir = 'images_train_test_val/train'\n",
    "output_dir = 'processed_images'\n",
    "\n",
    "# Apply preprocessing and save to new directory\n",
    "for category in os.listdir(input_dir):\n",
    "    category_path = os.path.join(input_dir, category)\n",
    "    if os.path.isdir(category_path):\n",
    "        output_category_path = os.path.join(output_dir, category)\n",
    "        if not os.path.exists(output_category_path):\n",
    "            os.makedirs(output_category_path)\n",
    "        for image_name in os.listdir(category_path):\n",
    "            image_path = os.path.join(category_path, image_name)\n",
    "            preprocessed_img = preprocess_image(image_path)\n",
    "            if preprocessed_img:\n",
    "                preprocessed_img.save(os.path.join(output_category_path, image_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7350 images belonging to 21 classes.\n",
      "Found 2100 images belonging to 21 classes.\n",
      "Found 1050 images belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/processed_images'\n",
    "val_dir = 'data/images_train_test_val/validation'\n",
    "test_dir = 'data/images_train_test_val/test'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 12:23:45.495640: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-07-19 12:23:45.495672: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-07-19 12:23:45.495681: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-07-19 12:23:45.495875: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-19 12:23:45.495887: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(21, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 19:40:18.819977: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/jay/anaconda3/envs/tf_m1_env/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - accuracy: 0.6862 - loss: 9.1536 \n",
      "Epoch 1: val_loss improved from inf to 11.08850, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2152s\u001b[0m 18s/step - accuracy: 0.6877 - loss: 9.1462 - val_accuracy: 0.0486 - val_loss: 11.0885\n",
      "Epoch 2/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.9850 - loss: 7.0426 \n",
      "Epoch 2: val_loss improved from 11.08850 to 9.87264, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2032s\u001b[0m 18s/step - accuracy: 0.9850 - loss: 7.0400 - val_accuracy: 0.0514 - val_loss: 9.8726\n",
      "Epoch 3/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9903 - loss: 5.7648 \n",
      "Epoch 3: val_loss improved from 9.87264 to 8.95772, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1711s\u001b[0m 15s/step - accuracy: 0.9903 - loss: 5.7621 - val_accuracy: 0.0724 - val_loss: 8.9577\n",
      "Epoch 4/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9921 - loss: 4.5540 \n",
      "Epoch 4: val_loss improved from 8.95772 to 8.00368, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1849s\u001b[0m 16s/step - accuracy: 0.9921 - loss: 4.5517 - val_accuracy: 0.0857 - val_loss: 8.0037\n",
      "Epoch 5/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.9931 - loss: 3.5089 \n",
      "Epoch 5: val_loss improved from 8.00368 to 5.99710, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1778s\u001b[0m 15s/step - accuracy: 0.9931 - loss: 3.5069 - val_accuracy: 0.2324 - val_loss: 5.9971\n",
      "Epoch 6/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9919 - loss: 2.6567 \n",
      "Epoch 6: val_loss improved from 5.99710 to 4.34997, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1904s\u001b[0m 17s/step - accuracy: 0.9919 - loss: 2.6552 - val_accuracy: 0.4390 - val_loss: 4.3500\n",
      "Epoch 7/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9904 - loss: 1.9996 \n",
      "Epoch 7: val_loss improved from 4.34997 to 2.82644, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1904s\u001b[0m 17s/step - accuracy: 0.9904 - loss: 1.9984 - val_accuracy: 0.6657 - val_loss: 2.8264\n",
      "Epoch 8/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9932 - loss: 1.4920 \n",
      "Epoch 8: val_loss improved from 2.82644 to 1.88951, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1827s\u001b[0m 16s/step - accuracy: 0.9932 - loss: 1.4912 - val_accuracy: 0.8429 - val_loss: 1.8895\n",
      "Epoch 9/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9815 - loss: 1.1709 \n",
      "Epoch 9: val_loss improved from 1.88951 to 1.29647, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1868s\u001b[0m 16s/step - accuracy: 0.9816 - loss: 1.1701 - val_accuracy: 0.9000 - val_loss: 1.2965\n",
      "Epoch 10/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9921 - loss: 0.8709 \n",
      "Epoch 10: val_loss improved from 1.29647 to 1.02975, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1805s\u001b[0m 16s/step - accuracy: 0.9921 - loss: 0.8705 - val_accuracy: 0.9057 - val_loss: 1.0298\n",
      "Epoch 11/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.9913 - loss: 0.6746 \n",
      "Epoch 11: val_loss improved from 1.02975 to 0.65350, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1958s\u001b[0m 17s/step - accuracy: 0.9913 - loss: 0.6743 - val_accuracy: 0.9571 - val_loss: 0.6535\n",
      "Epoch 12/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9960 - loss: 0.5084 \n",
      "Epoch 12: val_loss improved from 0.65350 to 0.63837, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1875s\u001b[0m 16s/step - accuracy: 0.9960 - loss: 0.5081 - val_accuracy: 0.9352 - val_loss: 0.6384\n",
      "Epoch 13/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9858 - loss: 0.4232 \n",
      "Epoch 13: val_loss improved from 0.63837 to 0.42813, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1913s\u001b[0m 17s/step - accuracy: 0.9858 - loss: 0.4231 - val_accuracy: 0.9667 - val_loss: 0.4281\n",
      "Epoch 14/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16s/step - accuracy: 0.9915 - loss: 0.3352 \n",
      "Epoch 14: val_loss did not improve from 0.42813\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1895s\u001b[0m 16s/step - accuracy: 0.9915 - loss: 0.3349 - val_accuracy: 0.9362 - val_loss: 0.4844\n",
      "Epoch 15/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.9940 - loss: 0.2511 \n",
      "Epoch 15: val_loss did not improve from 0.42813\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1915s\u001b[0m 17s/step - accuracy: 0.9940 - loss: 0.2510 - val_accuracy: 0.8971 - val_loss: 0.6102\n",
      "Epoch 16/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.9904 - loss: 0.2253 \n",
      "Epoch 16: val_loss did not improve from 0.42813\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1971s\u001b[0m 17s/step - accuracy: 0.9904 - loss: 0.2253 - val_accuracy: 0.8095 - val_loss: 1.0884\n",
      "Epoch 17/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.9907 - loss: 0.1802 \n",
      "Epoch 17: val_loss improved from 0.42813 to 0.22526, saving model to models/best_ResNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1975s\u001b[0m 17s/step - accuracy: 0.9907 - loss: 0.1801 - val_accuracy: 0.9657 - val_loss: 0.2253\n",
      "Epoch 18/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.9907 - loss: 0.1527 \n",
      "Epoch 18: val_loss did not improve from 0.22526\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1984s\u001b[0m 17s/step - accuracy: 0.9907 - loss: 0.1526 - val_accuracy: 0.9457 - val_loss: 0.3222\n",
      "Epoch 19/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - accuracy: 0.9935 - loss: 0.1206 \n",
      "Epoch 19: val_loss did not improve from 0.22526\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2094s\u001b[0m 18s/step - accuracy: 0.9935 - loss: 0.1205 - val_accuracy: 0.9333 - val_loss: 0.3605\n",
      "Epoch 20/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17s/step - accuracy: 0.9917 - loss: 0.1072 \n",
      "Epoch 20: val_loss did not improve from 0.22526\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2003s\u001b[0m 17s/step - accuracy: 0.9917 - loss: 0.1071 - val_accuracy: 0.9162 - val_loss: 0.5034\n",
      "Epoch 21/50\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - accuracy: 0.9918 - loss: 0.1016 \n",
      "Epoch 21: val_loss did not improve from 0.22526\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2107s\u001b[0m 18s/step - accuracy: 0.9918 - loss: 0.1015 - val_accuracy: 0.9619 - val_loss: 0.2393\n",
      "Epoch 22/50\n",
      "\u001b[1m 43/115\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5:53:46\u001b[0m 295s/step - accuracy: 0.9972 - loss: 0.0704"
     ]
    }
   ],
   "source": [
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/best_ResNet_model.keras', \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 12:23:55.137293: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "/Users/jay/anaconda3/envs/tf_m1_env/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 757ms/step - accuracy: 0.9903 - loss: 0.1562\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 863ms/step - accuracy: 0.9653 - loss: 0.2472\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 821ms/step - accuracy: 0.9689 - loss: 0.2128\n",
      "Train accuracy: 0.9910, Validation accuracy: 0.9681, Test accuracy: 0.9705\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model = tf.keras.models.load_model('models/best_ResNet_model.keras')\n",
    "train_loss, train_acc = model.evaluate(train_generator)\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "\n",
    "print(f'Train accuracy: {train_acc:.4f}, Validation accuracy: {val_acc:.4f}, Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# How can I load the model's history?\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The history object is a dictionary that contains the loss values and metric values during training\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# You can access it using the history attribute of the model object\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/best_ResNet_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plot training & validation accuracy values\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     agricultural       0.05      0.06      0.06        50\n",
      "         airplane       0.04      0.04      0.04        50\n",
      "  baseballdiamond       0.08      0.08      0.08        50\n",
      "            beach       0.08      0.08      0.08        50\n",
      "        buildings       0.05      0.06      0.05        50\n",
      "        chaparral       0.03      0.02      0.02        50\n",
      " denseresidential       0.07      0.06      0.06        50\n",
      "           forest       0.11      0.10      0.10        50\n",
      "          freeway       0.06      0.06      0.06        50\n",
      "       golfcourse       0.08      0.08      0.08        50\n",
      "           harbor       0.08      0.08      0.08        50\n",
      "     intersection       0.04      0.04      0.04        50\n",
      "mediumresidential       0.08      0.08      0.08        50\n",
      "   mobilehomepark       0.04      0.04      0.04        50\n",
      "         overpass       0.08      0.08      0.08        50\n",
      "       parkinglot       0.11      0.10      0.10        50\n",
      "            river       0.06      0.06      0.06        50\n",
      "           runway       0.06      0.06      0.06        50\n",
      "sparseresidential       0.04      0.04      0.04        50\n",
      "     storagetanks       0.10      0.10      0.10        50\n",
      "      tenniscourt       0.02      0.02      0.02        50\n",
      "\n",
      "         accuracy                           0.06      1050\n",
      "        macro avg       0.06      0.06      0.06      1050\n",
      "     weighted avg       0.06      0.06      0.06      1050\n",
      "\n",
      "[[3 5 3 1 5 3 2 1 5 1 2 0 3 2 2 2 1 2 0 4 3]\n",
      " [4 2 0 3 0 0 1 4 1 4 2 2 3 5 1 2 4 5 2 2 3]\n",
      " [4 3 4 3 4 2 1 1 3 3 3 2 2 3 0 2 1 2 0 3 4]\n",
      " [2 1 4 4 2 2 2 2 0 4 0 1 3 1 6 2 2 3 3 5 1]\n",
      " [0 1 3 3 3 1 1 5 3 3 3 2 2 4 3 2 2 0 3 0 6]\n",
      " [1 1 4 3 5 1 3 1 2 3 3 4 4 2 1 4 1 1 3 0 3]\n",
      " [2 0 5 2 5 2 3 0 1 0 3 7 0 2 4 1 2 4 2 3 2]\n",
      " [2 3 4 0 1 1 4 5 0 5 1 2 5 2 1 2 2 2 4 1 3]\n",
      " [3 3 1 3 3 1 1 3 3 1 3 4 2 4 2 2 3 2 2 2 2]\n",
      " [1 3 3 3 3 0 6 3 3 4 5 0 4 0 2 2 2 1 3 1 1]\n",
      " [5 0 5 3 2 1 2 1 1 2 4 3 3 2 4 2 2 1 2 2 3]\n",
      " [3 1 2 1 2 6 1 1 4 1 2 2 0 7 2 1 4 1 4 2 3]\n",
      " [0 4 2 4 1 2 1 2 6 3 3 1 4 0 3 1 3 2 2 3 3]\n",
      " [2 3 0 2 6 2 1 1 4 5 1 1 2 2 2 1 4 2 3 3 3]\n",
      " [7 1 1 1 2 0 2 1 4 3 3 1 1 3 4 1 1 2 4 2 6]\n",
      " [6 4 1 2 2 2 1 1 1 3 1 2 2 1 2 5 4 4 2 1 3]\n",
      " [0 5 2 1 3 2 2 1 3 1 4 2 0 1 3 3 3 5 1 4 4]\n",
      " [3 3 0 2 3 2 2 2 3 0 0 7 2 2 2 1 3 3 4 2 4]\n",
      " [3 3 2 2 1 3 1 5 1 1 4 3 1 0 2 4 4 3 2 2 3]\n",
      " [3 1 2 2 4 3 5 3 0 1 0 2 3 3 2 3 1 2 4 5 1]\n",
      " [4 3 2 4 3 1 3 3 1 2 4 2 3 5 2 3 0 0 2 2 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_generator.reset()\n",
    "Y_pred = model.predict(test_generator, len(test_generator))\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=test_generator.class_indices.keys()))\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model('models/best_ResNet_model.keras')\n",
    "\n",
    "train_loss, train_acc = model.evaluate(train_generator)\n",
    "print(f\"train_loss:{train_loss}:: train_accuracy: {train_acc}\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"val_loss:{val_loss}:: val_acc: {val_acc}\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"test_loss:{test_loss}:: test_acc: {test_acc}\")\n",
    "    \n",
    "predictions = model.predict(test_generator)\n",
    "predicted_label = np.argmax(predictions, axis=1)\n",
    "\n",
    "class_names = [\"Agricultural\", \"Airplane\", \"Baseballdiamond\", \"Beach\", \"Buildings\", \"Chaparral\", \"Denseresidential\", \n",
    "               \"Forest\", \"Freeway\", \"Golfcourse\", \"Harbor\", \"Intersection\", \"Mediumresidential\", \"Mobilehomepark\", \n",
    "               \"Overpass\", \"Parkinglot\", \"River\", \"Runway\", \"Sparseresidential\", \"Storagetanks\", \"Tenniscourt\"\n",
    "               ]\n",
    "\n",
    "true_labels = []\n",
    "for image, label in test_generator: #.as_numpy_iterator()\n",
    "    true_labels += list(label.numpy())\n",
    "\n",
    "# Plot confusion Matrix\n",
    "ModelUtils.plot_cm(y_true=true_labels, y_pred=predicted_label, class_names=class_names, save_path=cfg.Path.figure_save_path)\n",
    "# Plot Roc Auc Curve\n",
    "ModelUtils.multiclass_roc_auc_score(y_true=true_labels, model_predicted_label=predicted_label, class_names=class_names, save_path=cfg.Path.figure_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
