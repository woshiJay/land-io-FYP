{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow\n",
    "# pip install keras\n",
    "# pip install numpy\n",
    "# pip install matplotlib\n",
    "# pip install pandas\n",
    "# pip install scikit-learn\n",
    "# pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, size=(224, 224)):\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Quality enhancement\n",
    "    enhancer = ImageEnhance.Sharpness(img)\n",
    "    img = enhancer.enhance(2.0)  # Increase sharpness\n",
    "    \n",
    "    # Noise reduction\n",
    "    image = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Consistency\n",
    "    img = img.resize(size, Image.LANCZOS)\n",
    "    \n",
    "    # Normalization\n",
    "    img_array = np.array(img) / 255.0\n",
    "    \n",
    "    # Convert back to image\n",
    "    img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Apply preprocessing and save to new directory\n",
    "input_dir = 'data/images_train_test_val/train'\n",
    "output_dir = 'data/processed_images'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for category in os.listdir(input_dir):\n",
    "    category_path = os.path.join(input_dir, category)\n",
    "    if os.path.isdir(category_path):\n",
    "        output_category_path = os.path.join(output_dir, category)\n",
    "        if not os.path.exists(output_category_path):\n",
    "            os.makedirs(output_category_path)\n",
    "        for image_name in os.listdir(category_path):\n",
    "            image_path = os.path.join(category_path, image_name)\n",
    "            preprocessed_img = preprocess_image(image_path)\n",
    "            preprocessed_img.save(os.path.join(output_category_path, image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Image Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total corrupted images: 0\n"
     ]
    }
   ],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "def check_images(directory):\n",
    "    corrupted_images = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img.verify()  # This will raise an exception if the image is corrupted\n",
    "            except (UnidentifiedImageError, IOError) as e:\n",
    "                corrupted_images.append(file_path)\n",
    "                print(f\"Corrupted image: {file_path} - {e}\")\n",
    "    return corrupted_images\n",
    "\n",
    "corrupted_images = check_images('images')\n",
    "print(f\"Total corrupted images: {len(corrupted_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7350 images belonging to 21 classes.\n",
      "Found 2100 images belonging to 21 classes.\n",
      "Found 1050 images belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/processed_images'\n",
    "val_dir = 'data/images_train_test_val/validation'\n",
    "test_dir = 'data/images_train_test_val/test'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 16:06:06.606314: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-07-21 16:06:06.606370: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-07-21 16:06:06.606394: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-07-21 16:06:06.606852: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-21 16:06:06.606887: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)  # Add batch normalization layer\n",
    "predictions = Dense(21, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "class SaveHistoryCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, file_path):\n",
    "        super(SaveHistoryCallback, self).__init__()\n",
    "        self.file_path = file_path\n",
    "        # Create the file initially or clear existing file\n",
    "        if not os.path.exists(file_path):\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump({}, f)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        with open(self.file_path, 'r+') as f:\n",
    "            history = json.load(f)\n",
    "            for key, value in logs.items():\n",
    "                if key in history:\n",
    "                    history[key].append(value)\n",
    "                else:\n",
    "                    history[key] = [value]\n",
    "            f.seek(0)  # Go to the beginning of the file\n",
    "            json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68s/step - accuracy: 0.4498 - loss: 9.1953  \n",
      "Epoch 1: val_loss improved from inf to 9.88394, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7946s\u001b[0m 69s/step - accuracy: 0.4516 - loss: 9.1871 - val_accuracy: 0.0457 - val_loss: 9.8839\n",
      "Epoch 2/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32s/step - accuracy: 0.9114 - loss: 6.9431 \n",
      "Epoch 2: val_loss improved from 9.88394 to 9.35267, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3844s\u001b[0m 33s/step - accuracy: 0.9115 - loss: 6.9412 - val_accuracy: 0.0329 - val_loss: 9.3527\n",
      "Epoch 3/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35s/step - accuracy: 0.9583 - loss: 6.1003 \n",
      "Epoch 3: val_loss improved from 9.35267 to 9.04535, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4162s\u001b[0m 36s/step - accuracy: 0.9584 - loss: 6.0986 - val_accuracy: 0.0510 - val_loss: 9.0454\n",
      "Epoch 4/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31s/step - accuracy: 0.9788 - loss: 5.3003 \n",
      "Epoch 4: val_loss improved from 9.04535 to 8.00920, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3629s\u001b[0m 31s/step - accuracy: 0.9788 - loss: 5.2987 - val_accuracy: 0.1400 - val_loss: 8.0092\n",
      "Epoch 5/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31s/step - accuracy: 0.9801 - loss: 4.5656 \n",
      "Epoch 5: val_loss improved from 8.00920 to 5.98586, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3604s\u001b[0m 31s/step - accuracy: 0.9801 - loss: 4.5640 - val_accuracy: 0.4305 - val_loss: 5.9859\n",
      "Epoch 6/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30s/step - accuracy: 0.9879 - loss: 3.8548 \n",
      "Epoch 6: val_loss improved from 5.98586 to 4.90860, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3554s\u001b[0m 31s/step - accuracy: 0.9879 - loss: 3.8535 - val_accuracy: 0.5633 - val_loss: 4.9086\n",
      "Epoch 7/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30s/step - accuracy: 0.9928 - loss: 3.2170 \n",
      "Epoch 7: val_loss improved from 4.90860 to 3.91480, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3523s\u001b[0m 31s/step - accuracy: 0.9928 - loss: 3.2158 - val_accuracy: 0.6748 - val_loss: 3.9148\n",
      "Epoch 8/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30s/step - accuracy: 0.9925 - loss: 2.6565 \n",
      "Epoch 8: val_loss improved from 3.91480 to 2.55983, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3528s\u001b[0m 31s/step - accuracy: 0.9925 - loss: 2.6554 - val_accuracy: 0.9095 - val_loss: 2.5598\n",
      "Epoch 9/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29s/step - accuracy: 0.9946 - loss: 2.1713 \n",
      "Epoch 9: val_loss improved from 2.55983 to 1.91948, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3428s\u001b[0m 30s/step - accuracy: 0.9946 - loss: 2.1703 - val_accuracy: 0.9710 - val_loss: 1.9195\n",
      "Epoch 10/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31s/step - accuracy: 0.9964 - loss: 1.7536 \n",
      "Epoch 10: val_loss improved from 1.91948 to 1.53477, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3620s\u001b[0m 31s/step - accuracy: 0.9964 - loss: 1.7528 - val_accuracy: 0.9814 - val_loss: 1.5348\n",
      "Epoch 11/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33s/step - accuracy: 0.9967 - loss: 1.4050 \n",
      "Epoch 11: val_loss improved from 1.53477 to 1.23668, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3806s\u001b[0m 33s/step - accuracy: 0.9967 - loss: 1.4043 - val_accuracy: 0.9790 - val_loss: 1.2367\n",
      "Epoch 12/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49s/step - accuracy: 0.9945 - loss: 1.1225 \n",
      "Epoch 12: val_loss improved from 1.23668 to 0.99224, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5598s\u001b[0m 49s/step - accuracy: 0.9945 - loss: 1.1219 - val_accuracy: 0.9786 - val_loss: 0.9922\n",
      "Epoch 13/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28s/step - accuracy: 0.9964 - loss: 0.8889 \n",
      "Epoch 13: val_loss improved from 0.99224 to 0.78010, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3314s\u001b[0m 29s/step - accuracy: 0.9964 - loss: 0.8885 - val_accuracy: 0.9833 - val_loss: 0.7801\n",
      "Epoch 14/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31s/step - accuracy: 0.9952 - loss: 0.7081 \n",
      "Epoch 14: val_loss improved from 0.78010 to 0.65600, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3649s\u001b[0m 32s/step - accuracy: 0.9952 - loss: 0.7077 - val_accuracy: 0.9786 - val_loss: 0.6560\n",
      "Epoch 15/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32s/step - accuracy: 0.9937 - loss: 0.5662 \n",
      "Epoch 15: val_loss improved from 0.65600 to 0.53410, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3770s\u001b[0m 33s/step - accuracy: 0.9937 - loss: 0.5660 - val_accuracy: 0.9805 - val_loss: 0.5341\n",
      "Epoch 16/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35s/step - accuracy: 0.9944 - loss: 0.4570 \n",
      "Epoch 16: val_loss improved from 0.53410 to 0.45386, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4087s\u001b[0m 35s/step - accuracy: 0.9944 - loss: 0.4569 - val_accuracy: 0.9757 - val_loss: 0.4539\n",
      "Epoch 17/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36s/step - accuracy: 0.9954 - loss: 0.3724 \n",
      "Epoch 17: val_loss improved from 0.45386 to 0.35478, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4303s\u001b[0m 37s/step - accuracy: 0.9954 - loss: 0.3723 - val_accuracy: 0.9848 - val_loss: 0.3548\n",
      "Epoch 18/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35s/step - accuracy: 0.9950 - loss: 0.3091 \n",
      "Epoch 18: val_loss improved from 0.35478 to 0.29910, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4146s\u001b[0m 36s/step - accuracy: 0.9950 - loss: 0.3090 - val_accuracy: 0.9857 - val_loss: 0.2991\n",
      "Epoch 19/20\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37s/step - accuracy: 0.9940 - loss: 0.2588 \n",
      "Epoch 19: val_loss improved from 0.29910 to 0.24816, saving model to models/best_EffNet_model.keras\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4374s\u001b[0m 38s/step - accuracy: 0.9940 - loss: 0.2587 - val_accuracy: 0.9862 - val_loss: 0.2482\n",
      "Epoch 20/20\n",
      "\u001b[1m 26/115\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:02:18\u001b[0m 42s/step - accuracy: 0.9941 - loss: 0.2396"
     ]
    }
   ],
   "source": [
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/best_EffNet_model.keras', \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    mode='min',\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint, early_stopping, SaveHistoryCallback('models/effNet_history.json')]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must call `compile()` before using the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/best_EffNet_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(val_generator)\n\u001b[1;32m      7\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_m1_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_m1_env/lib/python3.10/site-packages/keras/src/trainers/trainer.py:993\u001b[0m, in \u001b[0;36mTrainer._assert_compile_called\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    992\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: You must call `compile()` before using the model."
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_weights('models/best_EffNet_model.keras')\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_acc = model.evaluate(train_generator)\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "\n",
    "print(f'Train accuracy: {train_acc:.4f}, Validation accuracy: {val_acc:.4f}, Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_generator.reset()\n",
    "Y_pred = model.predict(test_generator, len(test_generator))\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=test_generator.class_indices.keys()))\n",
    "cm = confusion_matrix(test_generator.classes, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for hosting in Streamlit\n",
    "model.save('models/efficientnet_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
